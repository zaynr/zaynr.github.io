---
title: 性能优化那些事
date: 2021-01-04 23:07:26
tags:
---

C++ 程序员，比起其他的程序员，可能骨子里更喜欢“性能”这个词。毕竟 C++ 难写又难编，包管理一塌糊涂。部署更是离谱，还要看 glibc 的版本（Java: 你在说啥）。所以“性能”二字便成为了一个类似遮羞布的存在了，哈哈。

lock free 被作为一种性能优化技术提出，主要是因为 mutex lock/unlock 时，会有 sys call，导致软中断。频繁的软中断，这几个保存 lwp context 压栈出栈的操作，并频繁让出、占用 cpu 的行为很容易就成了性能瓶颈。

一般情况下，优化程序的性能不外乎三点：CPU、内存、IO。

## CPU

CPU 可以用 perf 来看热点函数，哪些函数执行的次数最多、占用最多 CPU 时间片。火焰图依赖于推栈和符号表，对于整个进程，使用 perf 进行 profile 意义不大，因为多线程程序很容易有大量的时间片在 cond_wait(futex) 里。基于 linux lwp 的概念，可以深入到某几个业务线程里进行 profile。但是这只能提供一个大体的方向而已，因为函数名一般都会掉到 C++ 的标准库或者 kernel 的函数里，最后你还是得一行行代码去抠。顶多知道哪个函数被调用最多，是不是写了个什么很挫的循环在里面。

```bash
# active rec
perf record -g -p xxx,xxx sleep 600

# offcpu
perf probe -a futex
perf probe -x /lib64/libc.so.6 malloc
```

平时常用的 perf 指令如上，一般是在 profile 了指定线程后，发现很多 malloc/futex 等等的调用，则可以继续使用 perf probe 插桩。通过指定桩的调用栈打点来确认这些桩点都是如何进来的，提供进一步优化的依据。

对于我这几年的经验，一般不需要做什么把 string 的 key hash 成 int 以提高后续比较性能的事情。最多的还是数据结构选取的不合理，比如将所有的任务储存在一个 list 里，然后每次都去遍历那个 ```std::list<Task>``` 来找到需要停止的任务。一个 ```std::map<TaskID, Task>``` 就直接让性能提升了二十倍。

当然，能避免频繁的 sys call 最好还是避免掉，举个例子。

### 记录耗时惹出的事

比如：```gettimeofday()```，这个调用好像很多人没想到是一个 sys call。在一些性能敏感的场景里，例如对一张图片提取特征的耗时，会在多处打桩，以避免出现返回慢等问题时像无头苍蝇一样乱找。对于 c++，std::chrono 或者 gettimeofday 其实差异不大，都是一个 sys call，获取系统时间。

但是在高并发场景下，每秒几千几万张的图片送进来，打桩时频繁获取时间则成了一个很重的负担。这也是知道了结论以后的倒推，通过 perf 只能看到是在频繁获取时间，日志、打桩、染色 id 等场景都会涉及到时间的获取。只有在打桩的时候，才会持续的去获取时间戳，记录耗时。

既然知道是频繁的获取时间导致的性能问题，那么我们就把获取时间戳的这个行为从 sys call 降级为一个用户态的事情：单独起一个线程，通过 usleep 来获取精确到毫秒的时间戳。atomic + volatile，这两板斧下去，就把打桩的 sys call 成功降级为了一个 atoimc get。更进一步，其实这个时间戳只有一个 setter，多个 reader，只要更新频率够得上需要的精度，没有必要再包上一层 atomic。既然封装出了一个用户态时间获取，其他获取时间的场景也都可复用这个线程的数据。

当然，这个实现也有一个很明显的问题：太占 CPU 了。在 HPC 里，一个机器 64 或者 86 个核，这种单开一个线程没什么影响，但是要是移动端呢？

## 内存

现在的机器内存一般不会成为瓶颈。对于 C++，内存可优化的点一个是避免传参数时的隐式复制，一个就是避免过多的 cache miss。

### 隐式复制

没什么好说的，传参数的时候多用**引用**和**指针**吧。最好少用指针，因为你可能会忘了写判空，不知道什么时候就跑飞了。

更严重的是，经常有人上层做一个 vector 或者 shared_ptr，避免内存泄漏。但是下层接数据的函数参数又给写成了指针。那么传的时候，要么 vector.data()，要么 shared_ptr.get()。最后，一层层传下去的这个指针，指向的类早就析构掉了，而某个异步的回调还在用。这个指针指向的堆内存，可能还不会马上被其他 malloc 用掉，在一小段时间内是合法的，跑了一会儿后才飞掉。碰上这种垃圾代码导致的问题，自求多福吧。

### cache miss

其实这个东西归纳在 IO 里比较好。一般性能优化也不会抠到这里来。但还是说一说吧，这个东西

## IO

当涉及到 IO，很多东西就显得无趣起来了。一样，举个例子吧。

### NON_BLOCKING

一开始，之前的老哥用现成的 libuv 来实现一个 http server，做一个特征提取服务。会有很多的 client 向 server 发起链接，发送一张图片后，server 对图片提取特征再返回给 client。起了若干个 worker 线程，经典的 one thread per connection，从 server socket 里捞链接出来读图片、提取特征、再返回。

但是性能很差。

因为 GPU 提取图片特征的时候是做了一个 batch，送入一张图片不会马上提取，而是使用 (五十张 || 等待满 15ms) 这个条件来进行特征提取。在这个业务场景下，显然不可能根据 GPU batch 的图片张数来增加 worker 线程。而每个 thread 都来取个五十张呢？也不对，因为你无法感知到 pending 的 client socket 有多少，肯定会出现 worker thread 负载不均的情况。

这时候，就需要一个 scheduler 的出现了。不是 worker thread 自己去 pending socket 里捞，而是通过 scheduler 定期获取 pending socket，然后 round-robin 往里塞。把 server socket 变成 NON_BLOCKING，scheduler 从里面捞到 EAGAIN 为止，然后再塞到 worker 里。

哈哈，这不就是 reactor 模型吗。

当然，里面还涉及到很多细节。比如：scheduler 要一次全捞完吗？不全捞的话，一次捞多少呢？碰上 ddos 怎么办？由于内存到 GPU 走的 PCIE，worker 向 GPU 送图时需不需要也做一个 batch？

## 通用的高性能代码

性能优化这种事，其实是高度定制化的。特别是抠到最后，还会有内嵌汇编、magic number 等等奇技淫巧，就是为了快上那么 1ms。

不可能有一个人写出了性能特别高，代码又十分通用、简介的模块。里面肯定塞满了各种特例、对平台判断的 if else，等等。
