---
title: 性能优化那些事
date: 2021-03-04 23:07:26
tags:
---

C++ 程序员，比起其他的程序员，可能骨子里更喜欢“性能”这个词。毕竟 C++ 难写又难编，包管理一塌糊涂。部署更是离谱，还要看 glibc 的版本（Java: 你在说啥）。所以“性能”二字便成为了一个类似遮羞布的存在了，哈哈。

lock free 被作为一种性能优化技术提出，主要是因为 mutex lock/unlock 时，会有 sys call，导致软中断。但是何以见得，软中断这几个保存 lwp context 压栈出栈的操作就成了性能瓶颈？

一般情况下，优化程序的性能不外乎三点：CPU、内存、IO。

## CPU

CPU 可以用 perf 来看热点函数，哪些函数执行的次数最多、占用最多 CPU 时间片。但是这只能提供一个大体的方向而已，因为函数名一般都会掉到 C++ 的标准库里，最后你还是得一行行代码去抠。顶多知道哪个函数被调用最多，是不是写了个什么很挫的循环在里面。

对于我这几年的经验，一般不需要做什么把 string 的 key hash 成 int 以提高后续比较性能的事情。最多的还是数据结构选取的不合理，比如将所有的任务储存在一个 list 里，然后每次都去遍历那个 ```std::list<Task>``` 来找到需要停止的任务。一个 ```std::map<TaskID, Task>``` 就直接让性能提升了二十倍。

当然，能避免频繁的 sys call 最好还是避免掉，举个例子。

### 记录耗时惹出的事

比如：```gettimeofday()```，这个调用好像很多人没想到是一个 sys call。在一些性能敏感的场景里，比如对一张图片提取特征的耗时，会在多处打桩，以避免出现返回慢等问题时像无头苍蝇一样乱找。对于 c++，std::chrono 或者 gettimeofday 其实差异不大，都是一个 sys call，获取系统时间。

但是在高并发场景下，每秒几千几万张的图片送进来，打桩时频繁获取时间则成了一个很重的负担。这也是知道了结论以后的倒推，通过 perf 只能看到是在频繁获取时间，日志、打桩、染色 id 等场景都会涉及到时间的获取。只有在打桩的时候，才会持续的去获取时间戳，记录耗时。

既然知道是频繁的获取时间导致的性能问题，那么我们就把获取时间戳的这个行为从 sys call 降级为一个用户态的事情：单独起一个线程，通过 usleep 来获取精确到毫秒的时间戳。atomic + volatile，这两板斧下去，就把打桩的 sys call 成功降级为了一个 atoimc get。既然封装出了一个用户态时间获取，其他获取时间的场景也都可复用这个线程的数据。

## 内存

现在的机器内存一般不会成为瓶颈。对于 C++，内存可优化的点一个是避免传参数时的隐式复制，一个就是避免过多的 cache miss。

### 隐式复制

没什么好说的，传参数的时候多用**引用**和**指针**吧。最好少用指针，因为你可能会忘了写判空，不知道什么时候就跑飞了。

### cache miss

其实这个东西归纳在 IO 里比较好。一般性能优化也不会抠到这里来。

## IO

当涉及到 IO，很多东西就显得无趣起来了。一样，举个例子吧。

### NON_BLOCKING

一开始，之前的老哥用现成的 libuv 来实现一个 http server，做一个特征提取服务。会有很多的 client 向 server 发起链接，发送一张图片后，server 对图片提取特征再返回给 client。起了若干个 worker 线程，经典的 one thread per connection，从 server socket 里捞链接出来读图片、提取特征、再返回。

但是性能很差。

因为 GPU 提取图片特征的时候是做了一个 batch，送入一张图片不会马上提取，而是使用 (五十张 || 等待满 15ms) 这个条件来进行特征提取。在这个业务场景下，显然不可能根据 GPU batch 的图片张数来增加 worker 线程。而每个 thread 都来取个五十张呢？也不对，因为你无法感知到 pending 的 client socket 有多少，肯定会出现 worker thread 负载不均的情况。

这时候，就需要一个 scheduler 的出现了。不是 worker thread 自己去 pending socket 里捞，而是通过 scheduler round-robin 往里塞。把 server fd 变成 NON_BLOCKING，scheduler 从里面捞到 EAGAIN 为止，然后再塞到 worker 里。

哈哈，这不就是 reactor 模型吗。
